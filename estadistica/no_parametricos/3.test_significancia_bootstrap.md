# Test de significancia con bootstrap

Esta ficha no es solo sobre test de hipótesis. Es sobre algo más profundo: **aprender a construir distribuciones desde nuestros datos reales**. Porque cuando entendemos eso, el bootstrap deja de ser una técnica más, y se convierte en una llave para pensar estadísticamente sin fórmulas.

### El problema real: necesitamos una distribución, pero no queremos suponerla

En estadística clásica, se parte de suposiciones: "la media sigue una distribución normal", "la diferencia entre grupos se ajusta a una t de Student".

Pero ¿qué pasa si no queremos asumir ninguna forma? ¿Y si simplemente queremos **ver cómo se comporta nuestro estadístico en un universo sin efecto real**, usando solo los datos que tenemos?

Ahí es donde entra la magia del bootstrap: **te permite construir una distribución desde cero**, basada únicamente en tu muestra.

Primero observás tus datos reales.
Luego los modificás para simular el escenario nulo (por ejemplo, centrarlos).
Después hacés remuestreos con reemplazo.
Y al calcular el estadístico en cada remuestreo, lo que obtenés es una **distribución empírica**: una montaña construida con datos, no con fórmulas.

Ese es el corazón del método.

Imaginá que desarrollás una estrategia de trading, la probás sobre un año de datos, y te da una ganancia promedio de +7%. ¿Eso prueba que la estrategia es buena? ¿O pudo haber sido suerte?

En estadística clásica, esta pregunta se responde con test de hipótesis, fórmulas, y suposiciones de normalidad. Pero nosotros queremos hacerlo sin suponer nada. Queremos una forma de evaluar si ese resultado **es excepcional o no**, usando solo nuestros propios datos.

Ahí es donde entra el **test de significancia con bootstrap**.

### La lógica intuitiva

Si la estrategia **no tuviera ningún efecto real** (es decir, si fuera puro azar), ¿qué tan probable sería obtener un resultado como el que obtuvimos?

Con el bootstrap podemos simular **qué resultados serían esperables si la estrategia no funcionara realmente**, y ver si lo que observamos se destaca.

Esto se parece mucho a lo que hace el p-valor en la estadística clásica, pero sin fórmulas ni distribuciones teóricas. Lo hacemos desde abajo, **simulando la distribución nula**.

Y esto tiene una interpretación concreta: es como si ejecutaras **la misma estrategia 10.000 veces bajo condiciones donde no funciona**, y observaras qué tan seguido aparece un resultado tan bueno como el tuyo. Si eso pasa casi nunca, es buena señal. Si pasa todo el tiempo, tal vez fue solo suerte.

### Ejemplo aplicado

Tenés una serie de retornos diarios de una estrategia:

```
[+1%, -0.5%, +2%, +0.3%, -1%, +1.5%, -0.7%]
```

La media observada es de +0.7% diario. Querés saber si ese valor es significativamente mayor que lo que obtendrías con una estrategia sin efecto real.

Una forma de simular esa “estrategia sin efecto” es:

* Centrar los datos (restar la media a todos los valores, para que el promedio sea 0)
* Luego hacer bootstrap sobre esa serie centrada
* En cada muestra, calcular la media

Esto genera una distribución empírica de medias bajo la hipótesis nula (*no hay efecto*).

Luego simplemente observás:

* ¿Qué tan lejos está tu media real de esa distribución?
* ¿Cuántas muestras tienen una media tan extrema o más que la observada?

Eso te da una medida de significancia empírica: **cuán inusual es tu resultado si la estrategia no tuviera efecto real**.

Si el resultado se repite muy seguido entre las muestras simuladas, entonces no tiene nada de especial. Pero si se ve muy pocas veces —por ejemplo, en menos del 5%—, entonces podés concluir que **algo real está pasando**.

### ¿Y si no centramos los datos?

Otra opción es **no centrar** los datos antes de hacer bootstrap. En este caso, simplemente usamos la muestra tal como está y generamos muchas muestras bootstrap con reemplazo. En cada una calculamos la media (o el estadístico de interés) y luego comparamos el resultado observado con esa distribución.

Esto responde a otra pregunta:

> “¿Qué tan raro es este resultado comparado con otros resultados que podrían haber surgido de este mismo sistema?”

Esta lógica **no parte de una hipótesis nula explícita** como “la media es 0”, sino que toma los datos observados como representativos de la variabilidad del sistema, y se pregunta si lo que observamos es raro dentro de esa misma lógica.

Es decir:

* No estás construyendo una distribución nula como en el test formal
* Estás usando bootstrap como un **benchmark empírico** sobre tu propio sistema

Esto es útil si:

* No querés hacer un test estricto, sino tener una idea de cuán estable o confiable es tu estadístico
* Sos consciente de que tus datos podrían tener algún efecto real, y no querés anularlo al centrar

**Advertencia:** si tu muestra incluye sesgos o efectos reales, este enfoque podría subestimar la significancia, ya que no simula una situación nula, sino algo más parecido a “el sistema tal como lo tengo”.

### Casos extremos y ejemplos numéricos para entender mejor

**Caso 1: todos los valores son iguales.**

```
[5, 5, 5, 5, 5, 5, 5]
```

En este caso, cualquier muestra bootstrap también estará compuesta de puros 5. Entonces todas las medias serán 5, sin ninguna variabilidad. La distribución simulada será una línea vertical. Si observás una media distinta a 5, será inmediatamente significativa, porque no puede surgir de esa distribución.

**Caso 2: todos los valores son iguales excepto uno extremo.**

```
[5, 5, 5, 5, 5, 5, 20]
```

Acá el valor 20 es un **outlier**. Algunas muestras bootstrap lo incluirán, otras no. Entonces la distribución de medias será más ancha, sesgada, y reflejará el impacto del valor extremo. Si ese valor infló mucho la media observada, se notará que la mayoría de las muestras sin ese outlier no la alcanzan. El algoritmo “detecta” esta rareza sin necesidad de fórmulas.

Esto muestra cómo el bootstrap **es sensible a la estructura interna de los datos**: reconoce la variabilidad real y la refleja en sus resultados. No se deja engañar fácilmente por valores atípicos aislados.

### Ejemplo numérico paso a paso

Para entender por qué algunas medias bootstrap pueden superar la media observada incluso después de centrar los datos, vamos a un caso simple y concreto:

Supongamos que los retornos de tu estrategia fueron:

```
[3, 4, 5]
```

La media observada es 4.

Ahora queremos simular el escenario en que la estrategia **no tiene efecto real**. Para eso, centramos los datos restando la media:

```
[3 - 4, 4 - 4, 5 - 4] = [-1, 0, +1]
```

Esto nos da una muestra con media 0, lo que representa un sistema neutral (sin efecto).

Ahora hacemos bootstrap sobre estos datos centrados. Cada remuestreo toma 3 valores al azar, con reemplazo:

* Muestra 1: \[-1, -1, 0] → media = -0.67
* Muestra 2: \[+1, +1, +1] → media = +1.00
* Muestra 3: \[-1, 0, +1] → media = 0
* Muestra 4: \[+1, +1, 0] → media = +0.67

Como ves, **algunas muestras aleatorias generan medias positivas**, incluso bastante altas, aunque todo parta de una media cero. Esto ocurre porque la variabilidad original sigue viva.

Entonces, cuando comparás con la media observada original (que era 4), estás preguntando:

> “¿Con qué frecuencia estas combinaciones aleatorias logran un resultado tan alto como 4?”

Si eso ocurre en pocas muestras, tu media real **es rara** y tenés evidencia contra la hipótesis nula. Si ocurre muchas veces, entonces tu resultado **no destaca** frente a lo que podría surgir por puro azar.

---

### El corazón del método: construir una distribución con tus propios datos

En cualquier test de hipótesis, la pregunta central es:

> **¿Qué tan probable es obtener un resultado como el que observé si no hubiera ningún efecto real?**

Para responderla, necesitamos conocer **cómo se comportaría el estadístico de interés si H₀ fuera cierta**. A eso le llamamos "distribución bajo la hipótesis nula".

Y acá es donde está la diferencia fundamental entre los enfoques clásicos y los no paramétricos:

* **La estadística clásica** asume una forma teórica para esa distribución (como la t de Student), basada en supuestos como normalidad, varianzas iguales, etc.
* **El bootstrap**, en cambio, **simula esa distribución desde abajo**, usando únicamente tus datos. No asume forma, solo repite remuestreos para ver qué tan variable puede ser tu estadístico **si la media fuera cero**.

Esta es la clave que te permite extender el enfoque a otros problemas: **una vez que entendés cómo construir la distribución bajo H₀ sin fórmulas, podés hacer test para cualquier cosa: medias, medianas, máximos, drawdowns o percentiles.**

El método es siempre el mismo: construís la montaña, ves dónde cae tu valor, y evaluás si es raro.

---

### Comparación con el test clásico t

Para entender mejor el valor del enfoque bootstrap, conviene compararlo paso a paso con un test clásico, como el t-test:

| Paso                    | Test t clásico                            | Bootstrap                                         |
| ----------------------- | ----------------------------------------- | ------------------------------------------------- |
| 1. Hipótesis nula (H₀)  | La media poblacional es 0                 | La media esperada bajo remuestreo centrado es 0   |
| 2. Supuesto             | Normalidad de los datos                   | Ninguno                                           |
| 3. Estadístico          | t = (media observada - 0) / (desvío / √n) | media observada comparada contra medias simuladas |
| 4. Distribución bajo H₀ | Distribución t teórica                    | Distribución empírica de medias bootstrap         |
| 5. Cálculo de p-valor   | Área en la cola de la t                   | Proporción de medias bootstrap ≥ media observada  |
| 6. Rechazo de H₀ si     | p-valor < 0.05                            | p-valor empírico < 0.05                           |

El test t requiere que tus datos provengan de una población con distribución normal. Si esa condición se rompe, sus resultados se vuelven dudosos.

En cambio, el bootstrap **no hace ninguna suposición de forma**, porque **usa tus datos reales** para construir la distribución nula.

Esto lo convierte en una herramienta poderosa cuando:

* Tenés pocos datos
* Sospechás de asimetrías o outliers
* No querés confiar en fórmulas teóricas

---

### Interpretación del resultado

Si solo 3 de las 10.000 muestras bootstrap superaron tu media real, podés decir que:

* Hay un 0.03% de chance de que ese resultado se dé por azar bajo la hipótesis nula
* Es decir, **tu resultado es significativo con un p-valor empírico de 0.0003**

Cuanto más bajo el p-valor, más excepcional es tu resultado dentro del universo de posibilidades generadas por la simulación.

### Ventajas de este método

* No requiere normalidad ni varianzas iguales
* Funciona con cualquier estadístico (no solo la media)
* Es visual, comprensible y flexible
* Te permite explorar escenarios extremos y detectar valores atípicos con naturalidad

### Notas conceptuales

* Este método genera una distribución nula **empírica**, basada en remuestreos
* No usa fórmulas ni tablas: todo se simula
* Es una forma no paramétrica de test de hipótesis
* Puede ser de una cola o dos colas, según la hipótesis que se quiera probar
* El resultado se interpreta como un **p-valor empírico**
* Simular con bootstrap equivale a imaginar que ejecutás la estrategia miles de veces, y luego mirás cuán seguido aparece un resultado como el tuyo

---

En la próxima ficha veremos cómo usar el bootstrap para estimar otras medidas más allá de la media, como el drawdown, los percentiles o el payoff. Porque no todo se resume en promedios.



## El problema real: evaluar sin fórmulas

Imaginá que desarrollás una estrategia de trading, la probás sobre un año de datos, y te da una ganancia promedio de +7%. ¿Eso prueba que la estrategia es buena? ¿O pudo haber sido suerte?

En estadística clásica, esta pregunta se responde con test de hipótesis, fórmulas, y suposiciones de normalidad. Pero nosotros queremos hacerlo sin suponer nada. Queremos una forma de evaluar si ese resultado **es excepcional o no**, usando solo nuestros propios datos.

Ahí es donde entra el **test de significancia con bootstrap**.

## La lógica intuitiva

Si la estrategia **no tuviera ningún efecto real** (es decir, si fuera puro azar), ¿qué tan probable sería obtener un resultado como el que obtuvimos?

Con el bootstrap podemos simular **qué resultados serían esperables si la estrategia no funcionara realmente**, y ver si lo que observamos se destaca.

Esto se parece mucho a lo que hace el p-valor en la estadística clásica, pero sin fórmulas ni distribuciones teóricas. Lo hacemos desde abajo, **simulando la distribución nula**.

Y esto tiene una interpretación concreta: es como si ejecutaras **la misma estrategia 10.000 veces bajo condiciones donde no funciona**, y observaras qué tan seguido aparece un resultado tan bueno como el tuyo. Si eso pasa casi nunca, es buena señal. Si pasa todo el tiempo, tal vez fue solo suerte.

## Ejemplo

Tenés una serie de retornos diarios de una estrategia:

```
[+1%, -0.5%, +2%, +0.3%, -1%, +1.5%, -0.7%]
```

La media observada es de +0.7% diario. Querés saber si ese valor es significativamente mayor que lo que obtendrías con una estrategia sin efecto real.

Una forma de simular esa “estrategia sin efecto” es:

* Centrar los datos (restar la media a todos los valores, para que el promedio sea 0)

* Luego hacer bootstrap sobre esa serie centrada

* En cada muestra, calcular la media

Esto genera una distribución empírica de medias bajo la hipótesis nula (_no hay efecto_).

Luego simplemente observás:

* ¿Qué tan lejos está tu media real de esa distribución?

* ¿Cuántas muestras tienen una media tan extrema o más que la observada?

Eso te da una medida de significancia empírica: **cuán inusual es tu resultado si la estrategia no tuviera efecto real**.

Si el resultado se repite muy seguido entre las muestras simuladas, entonces no tiene nada de especial. Pero si se ve muy pocas veces —por ejemplo, en menos del 5%—, entonces podés concluir que **algo real está pasando**.

### ¿Y si no centramos los datos?

Otra opción es **no centrar** los datos antes de hacer bootstrap. En este caso, simplemente usamos la muestra tal como está y generamos muchas muestras bootstrap con reemplazo. En cada una calculamos la media (o el estadístico de interés) y luego comparamos el resultado observado con esa distribución.

Esto responde a otra pregunta:

> “¿Qué tan raro es este resultado comparado con otros resultados que podrían haber surgido de este mismo sistema?”

Esta lógica **no parte de una hipótesis nula explícita** como “la media es 0”, sino que toma los datos observados como representativos de la variabilidad del sistema, y se pregunta si lo que observamos es raro dentro de esa misma lógica.

Es decir:

* No estás construyendo una distribución nula como en el test formal

* Estás usando bootstrap como un **benchmark empírico** sobre tu propio sistema

Esto es útil si:

* No querés hacer un test estricto, sino tener una idea de cuán estable o confiable es tu estadístico

* Sos consciente de que tus datos podrían tener algún efecto real, y no querés anularlo al centrar

**Advertencia:** si tu muestra incluye sesgos o efectos reales, este enfoque podría subestimar la significancia, ya que no simula una situación nula, sino algo más parecido a “el sistema tal como lo tengo”.

### Casos extremos para entender mejor

**Caso 1: todos los valores son iguales.**

```
[5, 5, 5, 5, 5, 5, 5]
```

En este caso, cualquier muestra bootstrap también estará compuesta de puros 5. Entonces todas las medias serán 5, sin ninguna variabilidad. La distribución simulada será una línea vertical. Si observás una media distinta a 5, será inmediatamente significativa, porque no puede surgir de esa distribución.

**Caso 2: todos los valores son iguales excepto uno extremo.**

```
[5, 5, 5, 5, 5, 5, 20]
```

Acá el valor 20 es un **outlier**. Algunas muestras bootstrap lo incluirán, otras no. Entonces la distribución de medias será más ancha, sesgada, y reflejará el impacto del valor extremo. Si ese valor infló mucho la media observada, se notará que la mayoría de las muestras sin ese outlier no la alcanzan. El algoritmo “detecta” esta rareza sin necesidad de fórmulas.

Esto muestra cómo el bootstrap **es sensible a la estructura interna de los datos**: reconoce la variabilidad real y la refleja en sus resultados. No se deja engañar fácilmente por valores atípicos aislados.

### Interpretación del resultado

Si solo 3 de las 10.000 muestras bootstrap superaron tu media real, podés decir que:

* Hay un 0.03% de chance de que ese resultado se dé por azar bajo la hipótesis nula

* Es decir, **tu resultado es significativo con un p-valor empírico de 0.0003**

Cuanto más bajo el p-valor, más excepcional es tu resultado dentro del universo de posibilidades generadas por la simulación.

### Ventajas de este método

* No requiere normalidad ni varianzas iguales

* Funciona con cualquier estadístico (no solo la media)

* Es visual, comprensible y flexible

* Te permite explorar escenarios extremos y detectar valores atípicos con naturalidad

### Notas conceptuales

* Este método genera una distribución nula **empírica**, basada en remuestreos

* No usa fórmulas ni tablas: todo se simula

* Es una forma no paramétrica de test de hipótesis

* Puede ser de una cola o dos colas, según la hipótesis que se quiera probar

* El resultado se interpreta como un **p-valor empírico**

* Simular con bootstrap equivale a imaginar que ejecutás la estrategia miles de veces, y luego mirás cuán seguido aparece un resultado como el tuyo

***

En la próxima ficha veremos cómo usar el bootstrap para estimar otras medidas más allá de la media, como el drawdown, los percentiles o el payoff. Porque no todo se resume en promedios.

### El problema real: evaluar sin fórmulas

Imaginá que desarrollás una estrategia de trading, la probás sobre un año de datos, y te da una ganancia promedio de +7%. ¿Eso prueba que la estrategia es buena? ¿O pudo haber sido suerte?

En estadística clásica, esta pregunta se responde con test de hipótesis, fórmulas, y suposiciones de normalidad. Pero nosotros queremos hacerlo sin suponer nada. Queremos una forma de evaluar si ese resultado **es excepcional o no**, usando solo nuestros propios datos.

Ahí es donde entra el **test de significancia con bootstrap**.

### La lógica intuitiva

Si la estrategia **no tuviera ningún efecto real** (es decir, si fuera puro azar), ¿qué tan probable sería obtener un resultado como el que obtuvimos?

Con el bootstrap podemos simular **qué resultados serían esperables si la estrategia no funcionara realmente**, y ver si lo que observamos se destaca.

Esto se parece mucho a lo que hace el p-valor en la estadística clásica, pero sin fórmulas ni distribuciones teóricas. Lo hacemos desde abajo, **simulando la distribución nula**.

### Ejemplo aplicado

Tenés una serie de retornos diarios de una estrategia:

```
[+1%, -0.5%, +2%, +0.3%, -1%, +1.5%, -0.7%, ...]
```

Querés saber si el retorno promedio observado (por ejemplo, +0.7% diario) es significativamente mayor que lo que obtendrías con una estrategia sin efecto real.

Una forma de simular esa “estrategia sin efecto” es:

* Centrar los datos (restar la media a todos los valores, para que el promedio sea 0)

* Luego hacer bootstrap sobre esa serie centrada

* En cada muestra, calcular la media

Esto genera una distribución empírica de medias bajo la hipótesis nula (_no hay efecto_).

Luego simplemente observás:

* ¿Qué tan lejos está tu media real de esa distribución?

* ¿Cuántas muestras tienen una media tan extrema o más que la observada?

Eso te da una medida de significancia empírica: **cuán inusual es tu resultado si la estrategia no tuviera efecto real**.

### ¿Y si no centramos los datos?

Otra opción es **no centrar** los datos antes de hacer bootstrap. En este caso, simplemente usamos la muestra tal como está y generamos muchas muestras bootstrap con reemplazo. En cada una calculamos la media (o el estadístico de interés) y luego comparamos el resultado observado con esa distribución.

Esto responde a otra pregunta:

> “¿Qué tan raro es este resultado comparado con otros resultados que podrían haber surgido de este mismo sistema?”

Esta lógica **no parte de una hipótesis nula explícita** como “la media es 0”, sino que toma los datos observados como representativos de la variabilidad del sistema, y se pregunta si lo que observamos es raro dentro de esa misma lógica.

Es decir:

* No estás construyendo una distribución nula como en el test formal

* Estás usando bootstrap como un **benchmark empírico** sobre tu propio sistema

Esto es útil si:

* No querés hacer un test estricto, sino tener una idea de cuán estable o confiable es tu estadístico

* Sos consciente de que tus datos podrían tener algún efecto real, y no querés anularlo al centrar

**Advertencia:** si tu muestra incluye sesgos o efectos reales, este enfoque podría subestimar la significancia, ya que no simula una situación nula, sino algo más parecido a “el sistema tal como lo tengo”.

### Interpretación del resultado

Si solo 3 de las 1000 muestras bootstrap superaron tu media real, podés decir que:

* Hay un 0.3% de chance de que ese resultado se dé por azar bajo la hipótesis nula

* Es decir, **tu resultado es significativo con un p-valor empírico de 0.003**

### Ventajas de este método

* No requiere normalidad ni varianzas iguales

* Funciona con cualquier estadístico (no solo la media)

* Es visual, comprensible y flexible

### Notas conceptuales

* Este método genera una distribución nula **empírica**, basada en remuestreos

* No usa fórmulas ni tablas: todo se simula

* Es una forma no paramétrica de test de hipótesis

* Puede ser de una cola o dos colas, según la hipótesis que se quiera probar

* El resultado se interpreta como un **p-valor empírico**

***

En la próxima ficha veremos cómo usar el bootstrap para estimar otras medidas más allá de la media, como el drawdown, los percentiles o el payoff. Porque no todo se resume en promedios.
